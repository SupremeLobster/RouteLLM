{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_ENDPOINT = \"https://oai-dxclz-dev-oaicat-01.openai.azure.com\"\n",
    "API_KEY = \"b75c6627bded4f8dbe42825aaa5a1528\"\n",
    "API_VERSION = \"2023-07-01-preview\"\n",
    "LLM_DEPLOYMENT_NAME = \"GPT-4o\"\n",
    "LLM_MODEL_NAME = \"gpt-4o\"\n",
    "EMBEDDINGS_MODEL_NAME = \"text-embedding-3-large\"\n",
    "EMBEDDINGS_DEPLOYMENT_NAME = \"embedding-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_model():\n",
    "    return  AzureOpenAIEmbedding(\n",
    "        model=EMBEDDINGS_MODEL_NAME,\n",
    "        deployment_name=EMBEDDINGS_DEPLOYMENT_NAME,\n",
    "        api_key=API_KEY,\n",
    "        azure_endpoint=AZURE_ENDPOINT,\n",
    "        api_version=API_VERSION,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = create_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str):\n",
    "    return embed_model._get_query_embedding(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"routellm/gpt4_judge_battles\")\n",
    "# dataset_embeddings = load_dataset(\"routellm/gpt4_judge_battles_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b778190579604c3bbcff0b58c22c0ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/110 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "305427462"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save dataset to disk in json format, instead of the default parquet format\n",
    "dataset[\"train\"].to_json(\"data/gpt4_judge_battles.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset file, and process it line by line\n",
    "with open(\"data/gpt4_judge_battles.json\") as f:\n",
    "    data_array = []\n",
    "    for i, line in enumerate(f):\n",
    "        # if i == 2:\n",
    "        #     break\n",
    "\n",
    "        data = json.loads(line)\n",
    "\n",
    "        winner = \"model_a\" if data[\"winner_model_a\"] else \"model_b\"\n",
    "        winner = \"model_b\" if data[\"winner_tie\"] else winner # Bias the model towards using the WEAK model if there is a tie\n",
    "\n",
    "        data[\"winner\"] = winner\n",
    "\n",
    "        data_array.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data_array to disk in json format\n",
    "with open(\"data/gpt4_judge_battles_prepared.json\", \"w\") as f:\n",
    "    json.dump(data_array, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(embeddings, save_filename: str):\n",
    "    embeddings_loaded = []\n",
    "\n",
    "    with open(save_filename, \"rb\") as f:\n",
    "        # Load the embeddings from the file, if it exists\n",
    "        embeddings_loaded = np.load(f).tolist()\n",
    "\n",
    "    # Open file and save to the end\n",
    "    with open(save_filename, \"wb\") as f:\n",
    "        # Append the new embeddings to the existing embeddings\n",
    "        embeddings_loaded.extend(embeddings)\n",
    "        embeddings = []\n",
    "        embeddings_to_save = np.array(embeddings_loaded)\n",
    "        embeddings_loaded = []\n",
    "        np.save(f, embeddings_to_save)\n",
    "    embeddings = []\n",
    "    embeddings_loaded = []\n",
    "    embeddings_to_save = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch prompts into groups of 2000 with a generator\n",
    "def batch_prompts(prompts, batch_size=2000, start=0):\n",
    "    for i in range(start, len(prompts), batch_size):\n",
    "        yield prompts[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for the dataset, using the column \"prompt\"\n",
    "\n",
    "def generate_embeddings(dataset, batch_size=2000, start=0, save_filename: str = \"data/gpt4_judge_battles_embeddings.npy\"):\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "    # Generate embeddings for the dataset\n",
    "    embeddings = []\n",
    "    total_length = (len(dataset[\"train\"][\"prompt\"]) - start) // (batch_size + 1)\n",
    "    for i, batch in tqdm(enumerate(batch_prompts(dataset[\"train\"][\"prompt\"], batch_size, start)), total=total_length):\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            batch_embeddings = list(executor.map(get_embedding, batch))\n",
    "        \n",
    "        embeddings.extend(batch_embeddings)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Every 50 iterations, save the embeddings to disk\n",
    "        if (i + 1) % 50 == 0:\n",
    "            save_embeddings(embeddings, save_filename)\n",
    "            embeddings = []\n",
    "            \n",
    "    save_embeddings(embeddings, save_filename)\n",
    "    embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_THREADS = 12\n",
    "BATCH_SIZE = NUM_THREADS * (2 ** 3)\n",
    "START = 0\n",
    "SAVE_FILENAME = \"data/gpt4_judge_battles_embeddings_2.npy\"\n",
    "\n",
    "generate_embeddings(dataset, BATCH_SIZE, START, SAVE_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from file\n",
    "with open(\"data/gpt4_judge_battles_embeddings.npy\", \"rb\") as f:\n",
    "    embeddings = np.load(f, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109101"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"train\"][\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109101"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Matrix Factorization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'routellm/routers/matrix_factorization/train_matrix_factorization.py'], returncode=1)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call a python script\n",
    "import subprocess\n",
    "\n",
    "subprocess.run([\"python\", \"routellm/routers/matrix_factorization/train_matrix_factorization.py\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RouteLLM-RznIK_2n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
